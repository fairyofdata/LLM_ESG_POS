{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVO8VRZ3Nm1f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28227,
     "status": "ok",
     "timestamp": 1728120899009,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "EkT3-udEL1zD",
    "outputId": "4d5674b6-2e17-4ed4-926f-e36d14503968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           full_text  \\\n",
      "0  “주문량 넘쳐 규모 7배이상 확대”유럽시장 공략에 본격 투자 나서 LG화학-삼성SD...   \n",
      "1  머니투데이 고석용 기자 사람인 구직자 대상 조사 2위에 카카오, 5위 네이버구직자들...   \n",
      "2  입사 선호기업 상위권에 IT·벤처기업들이 도약하는 양상이다.구인구직 매칭플랫폼 사람...   \n",
      "3   '코스피 2196·코스닥 738' 코스피 지수가 2200선 안착에 또다시 실패했다...   \n",
      "4  -올해 주총 집중도 지난해 대비 완화-26일과 27일 주총 집중예상일 추가 지정 검...   \n",
      "\n",
      "                                             content  \n",
      "0  “주문량 넘쳐 규모 7배이상 확대”유럽시장 공략에 본격 투자 나서 LG화학-삼성SD...  \n",
      "1  머니투데이 고석용 기자 사람인 구직자 대상 조사 2위에 카카오, 5위 네이버구직자들...  \n",
      "2  입사 선호기업 상위권에 IT·벤처기업들이 도약하는 양상이다.구인구직 매칭플랫폼 사람...  \n",
      "3   '코스피 2196·코스닥 738' 코스피 지수가 2200선 안착에 또다시 실패했다...  \n",
      "4  -올해 주총 집중도 지난해 대비 완화-26일과 27일 주총 집중예상일 추가 지정 검...  \n"
     ]
    }
   ],
   "source": [
    "# Code that tracks the number of labels 1 generated by each company\n",
    "data = pd.read_csv('/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_filterbase.csv')\n",
    "\n",
    "# 1. If the 'full_text' column is empty, replace it with a 'Content' column\n",
    "If 'full_text' is empty using the .fillna () of pandas, fill it with the value of the 'Content' column\n",
    "data['full_text'] = data['full_text'].fillna(data['content'])\n",
    "data['full_text'] = data['content'].fillna(data['full_text'])\n",
    "\n",
    "# 3. 'FULL_TEXT' is not only empty, but also has a value, but it is also replaced.\n",
    "# Supply with a 'content' value even if it includes spaces or empty strings.\n",
    "data['full_text'] = data.apply(lambda row: row['content'] if pd.isna(row['full_text']) or row['full_text'].strip() == \"\" else row['full_text'], axis=1)\n",
    "data['content'] = data.apply(lambda row: row['full_text'] if pd.isna(row['content']) or row['content'].strip() == \"\" else row['content'], axis=1)\n",
    "\n",
    "# Confirmation of results\n",
    "print(data[['full_text', 'content']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1728120899009,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "kTfzTmxzZJ6y",
    "outputId": "b62b169f-53b6-4d40-fb6e-06585f03e80a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labelso</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "labelso\n",
       "0    169559\n",
       "1     12332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data extraction of Label 1 and the number of label 1 for each company\n",
    "data['labelso'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5304,
     "status": "ok",
     "timestamp": 1728120904304,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "a8DHT9WuHJeT",
    "outputId": "004fd6eb-c0e3-468f-c8dc-26b9ce3ca71f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7hhx-VpWw7z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "\n",
    "# Data should include 'full_text' and 'labelso' column\n",
    "# If not, proper pretreatment is required\n",
    "data = data[['full_text', 'labelso']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1728120904305,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "XFJEmbijaAS-",
    "outputId": "b5477d24-d634-42fa-e005-4590df781747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelso\n",
      "1    12332\n",
      "0    12332\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Remove the label 1 and the label 0\n",
    "label_1 = data[data['labelso'] == 1]\n",
    "label_0 = data[data['labelso'] == 0]\n",
    "\n",
    "#Label 0 randomly down Sampling according to the number of label 1\n",
    "label_0_downsampled = resample(label_0,\n",
    "                               replace=False,  # 복원 없이 샘플링\n",
    "                               n_samples=len(label_1),  # 라벨 1과 같은 수로 샘플링\n",
    "                               random_state=42)  # 재현성을 위한 random state 설정\n",
    "\n",
    "#Dopling Label 0 and Label 1 again combine\n",
    "data_balanced = pd.concat([label_1, label_0_downsampled])\n",
    "\n",
    "# Mix data\n",
    "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Output\n",
    "print(data_balanced['labelso'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WS5QyJehS-E5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Changed the label column name to 'label'\n",
    "data = data.rename(columns={'labelso': 'label'})\n",
    "\n",
    "# Since then, processing using 'label' in the process\n",
    "label_1 = data[data['label'] == 1]\n",
    "label_0 = data[data['label'] == 0]\n",
    "\n",
    "# Maintaining the same sampling and pipeline\n",
    "label_1_sampled = resample(label_1, n_samples=12332, random_state=42)\n",
    "label_0_sampled = resample(label_0, n_samples=12332, random_state=42)\n",
    "\n",
    "data_balanced = pd.concat([label_1_sampled, label_0_sampled])\n",
    "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data_balanced['full_text'].tolist(),\n",
    "    data_balanced['label'].tolist(),  # 'label'을 사용\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8226,
     "status": "ok",
     "timestamp": 1728123122294,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "G5ew3fjHS-DB",
    "outputId": "6d84f67e-cb61-476a-ece9-6af334425b76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "#KoelectRA model and torque knighter load\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=2)\n",
    "\n",
    "# 데이터 토크나이징 (max_length = 256)\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)# 평균 토큰 길이 계산 = 775로 확인함\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, train_labels)\n",
    "val_dataset = Dataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 1997329,
     "status": "ok",
     "timestamp": 1728125119621,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "T49SbfoaS-AD",
    "outputId": "32c9bdf4-05e8-4178-f13d-6b22c9f62764"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 32:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.673256</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.625200</td>\n",
       "      <td>0.616291</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.521127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.562249</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.592105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_koelectra/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_koelectra/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_koelectra/vocab.txt',\n",
       " '/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_koelectra/added_tokens.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "#KoelectRA model and torque knighter load\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=2)\n",
    "\n",
    "# 데이터 토크나이징 (max_length = 256)\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, train_labels)\n",
    "val_dataset = Dataset(val_encodings, val_labels)\n",
    "\n",
    "# Prevent VALUEERROR by converting all model parameters continuously\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.contiguous()\n",
    "\n",
    "# Definition of indicators for model evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# TRAINGARGUMENTS setting\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_koelectra',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # deprecated 된 evaluation_strategy 대신 eval_strategy 사용\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer setting\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics  # 평가 지표 추가\n",
    ")\n",
    "\n",
    "#Model learning\n",
    "trainer.train()\n",
    "\n",
    "#Save model after completion of learning\n",
    "model.save_pretrained('/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_koelectra')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/Kwargs/020. 전처리/A0_Sports_Obituary_koelectra')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "executionInfo": {
     "elapsed": 44247,
     "status": "ok",
     "timestamp": 1728127040919,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "KWP5WcFATOFs",
    "outputId": "da4386d2-d45c-4215-937b-4426a09dadec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.8653846153846154\n",
      "Recall: 0.45\n",
      "F1-score: 0.5921052631578947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Output of evaluation results\n",
    "metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Accuracy: {metrics['eval_accuracy']}\")\n",
    "print(f\"Precision: {metrics['eval_precision']}\")\n",
    "print(f\"Recall: {metrics['eval_recall']}\")\n",
    "print(f\"F1-score: {metrics['eval_f1']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3543,
     "status": "ok",
     "timestamp": 1728127190350,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "kLHQ7HuFTn8G",
    "outputId": "47dee074-0a2e-4181-a3d8-96c851fcadfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(엑스포츠뉴스 잠실, 박지영 기자) 5일 오후 서울 송파구 잠실야구장에서 열린 '2024 신한 SOL Bank KBO 포스트시즌' KT 위즈와 LG 트윈스의 준플레이오프 1차전 경기, 5회초 KT 선두타자 황재균이 헛스윙 삼진 아웃으로 물러나고 있다.  박지영 기자 jypark@xportsnews.com\n",
      "예측된 라벨: 1\n"
     ]
    }
   ],
   "source": [
    "# Text input and tokenization for testing\n",
    "test_text = input()\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Prediction using model\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Results label output\n",
    "label = predictions.item()\n",
    "print(f\"예측된 라벨: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4010,
     "status": "ok",
     "timestamp": 1728127253743,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "OdG7msKWaGfr",
    "outputId": "2ec8a8a6-3e06-4d42-bda0-d4f5dd7b6c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상 네이버페이 창 결제방식 다양, 피싱 사이트 ‘일반결제와 네이버페이 간편결제’ 두 가지만 서비스 피싱 사이트에서 무통장입금 거래 선택시 사기 계좌로 연결, 송금 시 물품 수령 불가능 누리랩, 네이버페이 피싱 사이트 분석 보고서 발표  [보안뉴스 김영명 기자] 네이버(NAVER)의 결제 시스템인 네이버페이(NAVER PAY)를 이용한 네이버쇼핑은 최근 쿠팡을 넘어 국내 점유율 22%를 기록하며 전자상거래 분야 시장점유율에서 1위를 차지했다. 네이버쇼핑 서비스는 물품의 구매와 결제 등을 매우 편리하게 만들어 주는 시스템이자 서비스다. 하지만 최근 결제 창 등을 사칭해 사용자의 정보 탈취 혹은 사기 행위를 벌이는 피싱 사이트가 다수 발견되어 사용자들의 주의가 필요하다.   ▲네이버페이 피싱 사이트의 최초 구성 화면[자료=누리랩 보안팀]  보안전문기업 누리랩이 발표한 ‘네이버페이 피싱 사이트 분석 보고서’를 살펴보면 올해 8월에 파악된 피싱 사이트의 URL 건수는 중복을 제외하고 전달보다 4만 7,177건이 증가한 8만 5,768건으로 나타났다.  해당 피싱 사이트에 처음으로 접속하면 사기 물품의 간략한 구매 정보와 함께 배송지를 등록하라는 메시지가 출력된다. 네이버쇼핑 구매 물품의 정식 결제 창과 매우 비슷한 사이트 구성을 띄고 있지만, 이를 자세히 살펴보면 여러 가지 차이점이 존재한다.  첫 번째 차이는 바로 사이트의 URL이다. 네이버페이의 공식 결제창 URL은 ‘or***.pay.nav**.com’ 형식이다. 하지만 피싱 사이트의 URL은 ‘naverpay-****.cafe’와 ‘naverpay-page****.cafe’로 표기된다. 또한 네이버쇼핑 공식 사이트의 실제 결제 창에서는 URL의 패스, 파라미터, 프레그먼트 등의 구성 요소에 판매자 상점 정보와 구매 물품 정보 등이 포함돼 있다. 하지만 피싱 사이트의 URL에서는 해당 정보 등을 확인할 수 없다. 따라서 URL 구성 화면부터 해당 사이트는 피싱이나 사기와 같은 악의적 행위를 목적으로 한 사이트임을 알 수 있다.\n",
      "예측된 라벨: 0\n"
     ]
    }
   ],
   "source": [
    "# Text input and tokenization for testing\n",
    "test_text = input()\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Prediction using model\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Results label output\n",
    "label = predictions.item()\n",
    "print(f\"예측된 라벨: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyL-aPbLBP45"
   },
   "outputs": [],
   "source": [
    "# Now, the data filtered into this model and discarded data classified as 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyML7AblUPdiCUH3K6XLr+ID",
   "machine_shape": "hm",
   "mount_file_id": "1bcWkgxg8zbccxxi7mMqCILyx2WCHBeor",
   "provenance": [
    {
     "file_id": "1bcWkgxg8zbccxxi7mMqCILyx2WCHBeor",
     "timestamp": 1727948330241
    },
    {
     "file_id": "1aqKgcNSifEsskejM5dSYeWbPYMnrY7gf",
     "timestamp": 1727827058219
    },
    {
     "file_id": "1rfaV-edX0QYlEJAouZAnGOro5cKhCzcQ",
     "timestamp": 1727632887793
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
