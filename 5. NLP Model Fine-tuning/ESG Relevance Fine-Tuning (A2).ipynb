{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLmpQnFg36V9"
   },
   "outputs": [],
   "source": [
    "# Necessary library installation\n",
    "!pip install transformers torch\n",
    "!pip install --upgrade gluonnlp pandas tqdm\n",
    "!pip install mxnet\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U\n",
    "\n",
    "#Library Import\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from google.colab import drive\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Google Drive Mount\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1535339,
     "status": "ok",
     "timestamp": 1725004388322,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "jkeICgA84Ho-",
    "outputId": "79fea87f-cfcc-4797-9c9b-a47c45964823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2355/2355 [06:33<00:00,  5.98it/s]\n",
      "Evaluating: 100%|██████████| 590/590 [00:31<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.4277\n",
      "Validation Loss: 1.3433\n",
      "Validation Accuracy: 0.4792\n",
      "Model and tokenizer saved to /content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_1 at Epoch 1\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2355/2355 [06:33<00:00,  5.98it/s]\n",
      "Evaluating: 100%|██████████| 590/590 [00:31<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6462\n",
      "Validation Loss: 1.6441\n",
      "Validation Accuracy: 0.4245\n",
      "Model and tokenizer saved to /content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_2 at Epoch 2\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2355/2355 [06:33<00:00,  5.98it/s]\n",
      "Evaluating: 100%|██████████| 590/590 [00:31<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6746\n",
      "Validation Loss: 1.6454\n",
      "Validation Accuracy: 0.4245\n",
      "Model and tokenizer saved to /content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_3 at Epoch 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# KoelectRa Talk Niser and Model\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "\n",
    "# Definition of model for classification\n",
    "class ElectraForCompanyClassification(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(ElectraForCompanyClassification, self).__init__()\n",
    "        self.electra = ElectraModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Data pretreatment function\n",
    "def preprocess(data, tokenizer, max_len=256):\n",
    "# Convert the Content column to a string list\n",
    "    texts = data[\"full_text\"].astype(str).tolist()\n",
    "# ESG_score converts columns into integers between 0 and 10\n",
    "    labels = (data[\"esg_score\"] * 10).astype(int).tolist()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)  # 분류를 위해 long 타입으로 변환\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "#Setting data file path\n",
    "file_paths = glob.glob('/content/drive/MyDrive/Kwargs/esg관련도/*.csv')\n",
    "\n",
    "#Model initialization\n",
    "model_name = \"monologg/koelectra-base-discriminator\"\n",
    "num_labels = 11  # 0부터 10까지의 11개의 클래스\n",
    "model = ElectraForCompanyClassification(model_name, num_labels)\n",
    "\n",
    "# Process all files\n",
    "all_train_datasets = []\n",
    "all_val_datasets = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "# Data pretreatment\n",
    "    inputs, labels = preprocess(data, tokenizer)\n",
    "\n",
    "# Tensordataset creation\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "\n",
    "# Dataset split\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    all_train_datasets.append(train_dataset)\n",
    "    all_val_datasets.append(val_dataset)\n",
    "\n",
    "# Combine all data\n",
    "train_dataset = torch.utils.data.ConcatDataset(all_train_datasets)\n",
    "val_dataset = torch.utils.data.ConcatDataset(all_val_datasets)\n",
    "\n",
    "# Create data loader\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n",
    "\n",
    "# setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return total_loss / len(val_dataloader), accuracy\n",
    "\n",
    "# Training and evaluation\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train(model, train_dataloader, optimizer, device)\n",
    "    val_loss, val_accuracy = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#Save model for each Epoch\n",
    "    epoch_output_dir = f\"/content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_{epoch + 1}\"\n",
    "\n",
    "# Create if there is no directory\n",
    "    if not os.path.exists(epoch_output_dir):\n",
    "        os.makedirs(epoch_output_dir)\n",
    "\n",
    "#Save model weight\n",
    "    torch.save(model.state_dict(), os.path.join(epoch_output_dir, \"pytorch_model.bin\"))\n",
    "    tokenizer.save_pretrained(epoch_output_dir)\n",
    "\n",
    "    print(f\"Model and tokenizer saved to {epoch_output_dir} at Epoch {epoch + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YYMl1aRHV0X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPdxKyxtW+AQHEk8OunKFN0",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1wwZt9Tzs8nbP_lPoyd951z5ndrY9jWh6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
