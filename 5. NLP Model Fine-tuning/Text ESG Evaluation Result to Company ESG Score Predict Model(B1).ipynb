{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9061,
     "status": "ok",
     "timestamp": 1725867604526,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "wGS1RWCAoDeL",
    "outputId": "bc0f95be-d14c-42a1-b20e-978a95839062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
      "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45383,
     "status": "ok",
     "timestamp": 1725867662194,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "mpfKT8SAoKOd",
    "outputId": "7d5121ea-7bdb-488e-87d4-f229e3e5824d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4i5mvxypEx8",
    "outputId": "82a794be-600b-4fcd-ff72-8a5daab3a113"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with max_len=128, batch_size=16, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:57<00:00,  5.25it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.8328\n",
      "Validation Loss: 2.8752\n",
      "Validation loss improved from inf to 2.8752. Saving model...\n",
      "Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:58<00:00,  5.14it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.4812\n",
      "Validation Loss: 2.3825\n",
      "Validation loss improved from 2.8752 to 2.3825. Saving model...\n",
      "Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:58<00:00,  5.15it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.9903\n",
      "Validation Loss: 2.3327\n",
      "Validation loss improved from 2.3825 to 2.3327. Saving model...\n",
      "Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:58<00:00,  5.16it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6659\n",
      "Validation Loss: 2.5637\n",
      "Validation loss did not improve. Patience counter: 1/5\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:58<00:00,  5.15it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.3392\n",
      "Validation Loss: 2.4069\n",
      "Validation loss did not improve. Patience counter: 2/5\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 303/303 [00:58<00:00,  5.15it/s]\n",
      "Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1364\n",
      "Validation Loss: 2.3098\n",
      "Validation loss improved from 2.3327 to 2.3098. Saving model...\n",
      "Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▋ | 262/303 [00:51<00:07,  5.27it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Data file path\n",
    "file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/final_dataset_test.csv'\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "X = data.drop(columns=['MSCI_Score'])\n",
    "\n",
    "# Y value is MSCI_SCORE\n",
    "y = data['MSCI_Score']\n",
    "\n",
    "#KoelectRA Talk Niser Road\n",
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "\n",
    "# Dataset class definition\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features, labels, tokenizer, max_len):\n",
    "# Features processing\n",
    "        if isinstance(features, pd.DataFrame):\n",
    "            self.features = features.reset_index(drop=True)\n",
    "        else:\n",
    "            self.features = features  # 리스트나 다른 형식일 경우 그대로 사용\n",
    "\n",
    "# Labels processing\n",
    "        if isinstance(labels, (pd.Series, pd.DataFrame)):\n",
    "            self.labels = labels.reset_index(drop=True)\n",
    "        else:\n",
    "            self.labels = labels  # 리스트나 다른 형식일 경우 그대로 사용\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "# Text and numerical features from features\n",
    "        if isinstance(self.features, pd.DataFrame):\n",
    "            text = self.features.iloc[idx]['full_text']\n",
    "            numeric_features = self.features.iloc[idx].drop(labels=['full_text', 'Company', 'date', 'Year']).astype(float).values\n",
    "        else:\n",
    "# Features is a list (for example, the dictionary of the list)\n",
    "            text = self.features[idx]['full_text']\n",
    "# 'FULL_TEXT', 'Company', 'Date', 'Yes'\n",
    "            numeric_features = [float(value) for key, value in self.features[idx].items() if key not in ['full_text', 'Company', 'date', 'Year']]\n",
    "            numeric_features = np.array(numeric_features)\n",
    "\n",
    "# Extract label from labels\n",
    "        if isinstance(self.labels, (pd.Series, pd.DataFrame)):\n",
    "            label = self.labels.iloc[idx]\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "\n",
    "# Text tokenization (Clean_UP_TOKENIZATION_SPACES removal)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'numeric_features': torch.tensor(numeric_features, dtype=torch.float),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Separation of learning set and verification set\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Setting values ​​for hyper parameter exploration\n",
    "max_len_values = [128, 256]\n",
    "batch_size_values = [16, 32]\n",
    "learning_rate_values = [2e-5, 3e-5, 5e-5]\n",
    "num_epochs = 50\n",
    "\n",
    "# Device setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Functions that convert tensors continuously\n",
    "def make_contiguous(model):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "# Training function definition\n",
    "def train(model, train_dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        numeric_features = batch['numeric_features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Text output of the koelectra model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze()\n",
    "\n",
    "# Text output and numerical feature combination\n",
    "        combined_output = torch.cat((logits.unsqueeze(1), numeric_features), dim=1)\n",
    "\n",
    "# Loss calculation (MSE Loss)\n",
    "        loss = F.mse_loss(combined_output.sum(dim=1), labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "# Evaluation function definition\n",
    "def evaluate(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numeric_features = batch['numeric_features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.squeeze()\n",
    "\n",
    "            combined_output = torch.cat((logits.unsqueeze(1), numeric_features), dim=1)\n",
    "            loss = F.mse_loss(combined_output.sum(dim=1), labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(combined_output.sum(dim=1).cpu().numpy())\n",
    "\n",
    "    return total_loss / len(val_dataloader), all_labels, all_preds\n",
    "\n",
    "# Hyper Parameter Navigation Loop\n",
    "for max_len in max_len_values:\n",
    "    for batch_size in batch_size_values:\n",
    "        for learning_rate in learning_rate_values:\n",
    "            print(f\"Training with max_len={max_len}, batch_size={batch_size}, learning_rate={learning_rate}\")\n",
    "\n",
    "# Creation of dataset objects\n",
    "            train_dataset = FeatureDataset(train_features, train_labels, tokenizer, max_len)\n",
    "            val_dataset = FeatureDataset(val_features, val_labels, tokenizer, max_len)\n",
    "\n",
    "#Dataloader\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Koelectra Model Road\n",
    "            model = ElectraForSequenceClassification.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=1)\n",
    "            model.to(device)\n",
    "            optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Variable for early termination\n",
    "            best_val_loss = float('inf')\n",
    "            patience = 5\n",
    "            patience_counter = 0\n",
    "\n",
    "# Training and evaluation\n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                train_loss = train(model, train_dataloader, optimizer, device)\n",
    "                val_loss, val_labels_out, val_preds = evaluate(model, val_dataloader, device)\n",
    "                print(f\"Training Loss: {train_loss:.4f}\")\n",
    "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model...\")\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "\n",
    "#Save the best model\n",
    "                    best_model_dir = f\"/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len{max_len}_batch{batch_size}_lr{learning_rate}\"\n",
    "                    if not os.path.exists(best_model_dir):\n",
    "                        os.makedirs(best_model_dir)\n",
    "                    make_contiguous(model)\n",
    "                    model.save_pretrained(best_model_dir)\n",
    "                    tokenizer.save_pretrained(best_model_dir)\n",
    "                    print(f\"Best model and tokenizer saved to {best_model_dir}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered. Stopping training...\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96076,
     "status": "ok",
     "timestamp": 1723794660842,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "oHfFgarr3Ngj",
    "outputId": "5fd1cfcd-d4a2-41b1-db9e-52eb1e65ce3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 441/441 [01:25<00:00,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label distribution:\n",
      "(3.645, 4.828]      1533\n",
      "(1.279, 2.462]      1502\n",
      "(4.828, 6.011]      1348\n",
      "(2.462, 3.645]       955\n",
      "(0.0965, 1.279]      873\n",
      "(6.011, 7.194]       509\n",
      "(-1.086, 0.0965]     155\n",
      "(7.194, 8.377]       148\n",
      "(8.377, 9.56]         15\n",
      "(-2.282, -1.086]      12\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Save model and torque nisor path\n",
    "output_dir = \"/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/\"\n",
    "\n",
    "#Model Road\n",
    "model = ElectraForSequenceClassification.from_pretrained(output_dir)\n",
    "model.eval()  # 평가 모드로 전환\n",
    "\n",
    "#Talk Nizor Road\n",
    "tokenizer = ElectraTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Read a new CSV file\n",
    "new_file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/sample_dataset_test.csv'\n",
    "new_data = pd.read_csv(new_file_path)\n",
    "\n",
    "# Data set class definition (the same as the class used in training)\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features, tokenizer, max_len):\n",
    "        self.features = features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "# Separate text and other numerical features\n",
    "        text = self.features.iloc[idx]['full_text']\n",
    "\n",
    "# Select only numerical features (select only numeric data)\n",
    "        numeric_features = self.features.iloc[idx].drop(labels=['full_text', 'Company', 'date', 'Year']).astype(float).values\n",
    "\n",
    "# Text tokenization\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "# Return\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'numeric_features': torch.tensor(numeric_features, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "# Parameter setting\n",
    "max_len = 128\n",
    "batch_size = 16\n",
    "\n",
    "# Creation of dataset objects\n",
    "predict_dataset = FeatureDataset(new_data, tokenizer, max_len)\n",
    "\n",
    "#Dataloader\n",
    "predict_dataloader = DataLoader(predict_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#Depation of predictive functions\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numeric_features = batch['numeric_features'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.squeeze()\n",
    "\n",
    "            combined_output = torch.cat((logits.unsqueeze(1), numeric_features), dim=1)\n",
    "            final_output = combined_output.sum(dim=1)  # 예: 간단히 합산하여 최종 출력 계산\n",
    "\n",
    "            predictions.extend(final_output.cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Prediction performance\n",
    "predictions = predict(model, predict_dataloader, device)\n",
    "\n",
    "# Check for predictive value distribution\n",
    "predicted_label_distribution = pd.Series(predictions).value_counts(bins=10)\n",
    "print(\"Predicted label distribution:\")\n",
    "print(predicted_label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1723794671130,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "RI2U6NIgKy1M",
    "outputId": "cf22bbaf-7b07-4ce6-abcc-daf18c4293d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in predictions: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert the prediction result to Pandas Series\n",
    "predictions_series = pd.Series(predictions)\n",
    "\n",
    "# Confirmation check\n",
    "missing_values = predictions_series.isnull().sum()\n",
    "print(f\"Number of missing values in predictions: {missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1723794680413,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "J1TH1C613VOV",
    "outputId": "0df68bc8-c292-4547-d638-42aee8aff676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /content/drive/MyDrive/Kwargs/모델B1/결과/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Add the prediction result to the data frame\n",
    "new_data['predicted_label'] = predictions\n",
    "\n",
    "#Save data containing predictive results\n",
    "output_file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/결과/predictions.csv'\n",
    "new_data.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Predictions saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1723794689217,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "n2V7yPsCHj_t",
    "outputId": "5da71ca8-883b-437e-e507-2117bc7a4e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Company  Year  average_label\n",
      "0       KB금융  2019       5.016282\n",
      "1       KB금융  2020       5.109671\n",
      "2       KB금융  2021       5.996064\n",
      "3       KB금융  2022       6.966534\n",
      "4       KB금융  2023       7.012989\n",
      "5      NAVER  2019       4.815658\n",
      "6      NAVER  2020       4.797409\n",
      "7      NAVER  2021       6.733145\n",
      "8      NAVER  2022       6.642238\n",
      "9      NAVER  2023       5.643522\n",
      "10    SK하이닉스  2019       2.097431\n",
      "11    SK하이닉스  2020       3.649572\n",
      "12    SK하이닉스  2021       3.710508\n",
      "13    SK하이닉스  2022       4.903346\n",
      "14    SK하이닉스  2023       4.920477\n",
      "15        기아  2019       1.796560\n",
      "16        기아  2020       1.301239\n",
      "17        기아  2021       2.058400\n",
      "18        기아  2022       1.960288\n",
      "19        기아  2023       2.774900\n",
      "20     삼성SDI  2019       4.350137\n",
      "21     삼성SDI  2020       4.419508\n",
      "22     삼성SDI  2021       4.213435\n",
      "23     삼성SDI  2022       4.739213\n",
      "24     삼성SDI  2023       4.776360\n",
      "25      삼성물산  2019       3.249006\n",
      "26      삼성물산  2020       4.275316\n",
      "27      삼성물산  2021       4.703798\n",
      "28      삼성물산  2022       4.477484\n",
      "29      삼성물산  2023       4.801013\n",
      "30  삼성바이오로직스  2019       2.390249\n",
      "31  삼성바이오로직스  2020       3.275905\n",
      "32  삼성바이오로직스  2021       2.203565\n",
      "33  삼성바이오로직스  2022       2.277998\n",
      "34  삼성바이오로직스  2023       1.842113\n",
      "35      삼성생명  2019       1.869106\n",
      "36      삼성생명  2020       3.027084\n",
      "37      삼성생명  2021       4.745852\n",
      "38      삼성생명  2022       4.388641\n",
      "39      삼성생명  2023       2.771990\n",
      "40      삼성전자  2019       4.225913\n",
      "41      삼성전자  2020       5.421561\n",
      "42      삼성전자  2021       5.118524\n",
      "43      삼성전자  2022       4.948362\n",
      "44      삼성전자  2023       5.915540\n",
      "45      셀트리온  2019       2.846863\n",
      "46      셀트리온  2020       2.165246\n",
      "47      셀트리온  2021       1.668776\n",
      "48      셀트리온  2022       0.632931\n",
      "49      셀트리온  2023       0.620857\n",
      "50      신한지주  2019       5.687663\n",
      "51      신한지주  2020       5.470094\n",
      "52      신한지주  2021       5.464048\n",
      "53      신한지주  2022       5.390045\n",
      "54      신한지주  2023       5.971607\n",
      "55       카카오  2019       3.345765\n",
      "56       카카오  2020       3.933448\n",
      "57       카카오  2021       4.670168\n",
      "58       카카오  2022       5.936382\n",
      "59       카카오  2023       4.776292\n",
      "60   포스코 홀딩스  2019       3.363093\n",
      "61   포스코 홀딩스  2020       3.434434\n",
      "62   포스코 홀딩스  2021       3.483677\n",
      "63   포스코 홀딩스  2022       3.435544\n",
      "64   포스코 홀딩스  2023       4.437256\n",
      "65     현대모비스  2019       0.538835\n",
      "66     현대모비스  2020       1.869277\n",
      "67     현대모비스  2021       1.215652\n",
      "68     현대모비스  2022       1.194249\n",
      "69     현대모비스  2023       1.688000\n",
      "70       현대차  2019       1.757339\n",
      "71       현대차  2020       1.671593\n",
      "72       현대차  2021       1.222235\n",
      "73       현대차  2022       1.381263\n",
      "74       현대차  2023       2.590015\n"
     ]
    }
   ],
   "source": [
    "# CSV file path\n",
    "file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/결과/predictions.csv'\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select only the necessary columns\n",
    "data = data[['Year', 'Company', 'predicted_label']]\n",
    "\n",
    "#Modter with Company and Yes, then the average calculation of predicted_label\n",
    "grouped_data = data.groupby(['Company', 'Year']).agg(average_label=('predicted_label', 'mean')).reset_index()\n",
    "print(grouped_data.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1723794696332,
     "user": {
      "displayName": "백현지",
      "userId": "02510457579208942463"
     },
     "user_tz": -540
    },
    "id": "TvA6so_CIFPN",
    "outputId": "a5f4bcb6-84dd-44d0-9277-d1f4b00c3873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 /content/drive/MyDrive/Kwargs/모델B1/결과/average_predictions.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "#Save the result as a new CSV file\n",
    "output_file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/결과/average_predictions.csv'\n",
    "grouped_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"결과가 {output_file_path}에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnIxrKXBf68LxDtYeEJsKZ",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
